{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Giới thiệu\n",
    "\n",
    "Trong bài toán linear regression dưới đây, ta sẽ có một vector chứa các thuộc tính và nhãn. Ta cần đi tìm một bộ tham số, hay hệ số tương ứng với các thuộc tính, để cho hàm mất mát trên tập dữ liệu là nhỏ nhất.<br>\n",
    "\n",
    "Tham khảo lý thuyết toán và cách sử dụng đại số tuyến tính để tìm lời giải:<br>\n",
    "[Machine Learning cơ bản - Linear Regression](https://machinelearningcoban.com/2016/12/28/linearregression/)\n",
    "\n",
    "<br>\n",
    "\n",
    "Trong bài này chúng ta sẽ tìm hiểu về cách sử dụng đạo hàm để tìm lời giải cho bài toán linear regression.\n",
    "\n",
    "<img align=\"left\" src=\"derivative.png\" >\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "<img align=\"left\" src=\"derivative_1.png\" >\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    " Trong hình trên, khi muốn di chuyển tới điểm x làm cho hàm có giá trị đạt cực tiểu (có thể là local hoặc global) thì ta cần phải đi ngược hướng đạo hàm. Thật vậy, khi đạo hàm dương, tức là hàm đồng biến, thì ta sẽ trừ x đi 1 lượng và do $x^{'} < x_{0}$ nên $f(x^{'}) < f(x_{0})$. Ngược lại thì ta sẽ cộng với x một lượng khi hàm nghịch biến.<br>\n",
    " Vậy ta có công thức cập nhật:<br>\n",
    " $x = x - \\eta {f}'(x) $ \n",
    " <br>\n",
    " với $\\eta$ là hệ số học hay **learning rate**\n",
    " <img align=\"left\" src=\"derivative_2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"derivative_3.png\" >\n",
    " \n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "Như đã trình bày ở trên, ta sẽ đi tìm một bộ tham số **w** để ứng với các thuộc tính $x_{i}$. Ở đây ta cộng thêm một đại lượng **b** gọi là bias, bộ tham số **w** cần tìm trong trường hợp có **b** là $w' = [w_{1},...,w_{n},1]$<br>\n",
    "Sử dụng quy tắc đạo hàm của hàm lỗi (error function) đối với hai biến **w,b** ta có được đạo hàm như công thức ở trên.\n",
    "<img align=\"left\" src=\"derivative_4.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"derivative_example.png\" >\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "Chúng ta sẽ xét ví dụ đơn giản để hiểu cách hoạt động của phương pháp trên<br>\n",
    "Đầu tiên chúng ta khởi tạo giá trị bất kỳ cho b và w, ví dụ $b=0.04$ và $w=-0.34$. Với bộ dòng dữ liệu đầu tiên. Area: 6.7 (x), Price: 9.1 (y). Ta có:<br>\n",
    "$𝑧=𝑥𝑤+𝑏= -2.238$ và $\\hspace{0.1cm}loss = (z-y)^{2} = 128.55$<br>\n",
    "Sử dụng công thức ở trên với $\\eta = 0.01$ ta có:<br>\n",
    "$w_{1} = w_{0} - \\eta L'_{w_{0}} = -0.34 - 0.01*2*6.7*(-2.238-9.1) = 1.17929$<br>\n",
    "Tương tự: $b_{1} = 0.26676$ <br>\n",
    "\n",
    "Và kết quả là giá trị hàm loss function giảm đáng kể. Xem hình bên dưới:\n",
    "<img align=\"left\" src=\"derivative_example_1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở ví dụ trên, ta dùng dạng 1-sample training, tức là ta lấy lần lượt từng dòng dữ liệu ra và sử dụng phương pháp cập nhật lại các hệ số nhờ vào đạo hàm, tuy giá trị của hàm mất mát có xu hướng giảm nhưng kết quả không được tốt vì với mỗi bộ dữ liệu đầu vào thì các hệ số b, w được điều chỉnh sẽ có xu hướng thiên về giá trị b, w nào đó mà làm cho bộ giá trị đầu vào mới là tốt hơn, có thể làm cho hàm mất mát của bộ giá trị đầu vào trước đó tăng lên, nhưng nói chung vẫn giảm được hàm mất mát cho toàn bộ dữ liệu so với tập giá trị b, w ban đầu<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "Với trường hợp sử dụng n-sample training thì ta cũng làm tương tự như trên, ta lấy ra n bộ dữ liệu và thực hiện tính đạo hàm như trên cho n bộ dữ liệu, thu được một vector $R_{n}$, rồi tính trung bình cộng các giá trị đạo hàm này và sử dụng giá trị này tương tự như giá trị đạo hàm trong trường hợp 1-sample training. Xem công thức ở hình phía dưới.\n",
    "<img align=\"left\" src=\"derivative_example_2.png\" >\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "Đó là giải thích cơ bản về cách hoạt động sử dụng đạo hàm để giải quyết bài toán linear regression, đây là dạng đơn giản do bài toán này có rất nhiều vấn đề cần giải quyết, tham khảo thêm trong sách Machine Learning cơ bản. Xem thêm ví dụ và code liên quan được đính kèm theo (Author: AI VIETNAM - AI Foundation Course)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
